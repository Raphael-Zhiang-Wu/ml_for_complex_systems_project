import time
import numpy as np
import torch
import math
from scipy.stats import multivariate_normal as normal
from collections import namedtuple
import matplotlib.pyplot as plt
from pres_config_file import Config

LAMBDA = 0.001 # was 0.05

# Set default tensor type to float
torch.set_default_dtype(torch.float32)

# Check for available devices 
device = torch.device("cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu")
print(f"Using device: {device}")

class Solver(object):
    def __init__(self, x_0_value=1.0):
        self.config = Config()
        
        self.valid_size = self.config.valid_size
        self.batch_size = self.config.batch_size
        self.num_iterations = self.config.num_iterations
        self.logging_frequency = self.config.logging_frequency
        self.lr_values = self.config.lr_values
        self.lr_boundaries = self.config.lr_boundaries
        self.x_0_value = x_0_value

        self.model = WholeNet().to(device)  # Move model to the selected device
        print("y_intial: ", self.model.p_init.detach().cpu().numpy())
        print("x_0_value: ", x_0_value)
        
        # PyTorch doesn't have PiecewiseConstantDecay directly, so we'll use a custom scheduler
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr_values[0], eps=1e-8)
        self.scheduler = torch.optim.lr_scheduler.MultiStepLR(
            self.optimizer, 
            milestones=self.lr_boundaries, 
            gamma=1.0  # No decrease since we manually set the learning rates
        )
        
        # Custom adjustment for learning rates at boundaries
        self.original_lr = self.lr_values[0]

    def train(self):
        """Training the model"""
        start_time = time.time()
        training_history = []
        validation_data = self.config.sample(self.valid_size)
        
        for step in range(self.num_iterations+1):
            # Custom learning rate adjustment
            if step in self.lr_boundaries:
                idx = self.lr_boundaries.index(step) + 1
                for param_group in self.optimizer.param_groups:
                    param_group['lr'] = self.lr_values[idx]
            
            # Calculate validation loss and log it
            if step % self.logging_frequency == 0:
                with torch.no_grad():
                    loss, performance, ratio = self.model(validation_data)
                    p_init_euclidean = np.linalg.norm(self.model.p_init.detach().cpu().numpy())
                    elapsed_time = time.time() - start_time
                    training_history.append([step, performance.item(), p_init_euclidean, loss.item(), ratio.item()])
                    print(f"step: {step:5d}, loss: {loss.item():.4e}, ||Y0||: {p_init_euclidean:.4e}, performance: {performance.item():.4e}, "
                          f"elapsed time: {int(elapsed_time):3d}, ratio: {ratio.item():.4e}")
            
            # Gradient descent
            self.optimizer.zero_grad()
            loss, _, _ = self.model(self.config.sample(self.batch_size))
            loss.backward()
            self.optimizer.step()

            self.scheduler.step()
            self.training_history = training_history
        
    def generate_trajectoy(self, sample_size, x_0_value = 1.0):
        """Generate the trajectory of the model. start from the initial value of X0 and apply the control generated by the FFNetU"""
        
        # Generate sample data for simulation
        sample_data = self.config.sample(sample_size)
        delta_W_TBW, _, jump_mask_TBLC, jump_sizes_BLCX = sample_data
        
        X_BX = self.config.X_init.repeat(sample_size, 1) * x_0_value  # Shape: (sample_size, dim_X)
        trajectory = []
        trajectory.append(X_BX.detach().cpu().numpy())  # Store initial state

        for t in range(self.config.time_step_count):
            current_time = t * self.config.delta_t
            u_BU = self.model.u_net((current_time, X_BX))  # Access u_net through self.model
            
            # Update X using drift
            X_BX = X_BX + self.config.drift(current_time, X_BX, u_BU) * self.config.delta_t
            
            # Add diffusion term
            X_BX = X_BX + torch.einsum('bxw,bw->bx', 
                                       self.config.diffusion(current_time, X_BX, u_BU), 
                                       delta_W_TBW[t, :, :])
            
            # Add jump terms
            for l in range(self.config.dim_L):
                X_BX = X_BX + u_BU * torch.einsum('bc,bcx->bx', 
                                                 jump_mask_TBLC[t, :, l, :], 
                                                 jump_sizes_BLCX[:, l, :, :])
                # Compensation term
                X_BX = X_BX - self.config.jump_intensity[l] * (np.exp(self.config.log_normal_mu[l] + 0.5 * self.config.log_normal_sigma[l] ** 2) - 1) * u_BU * self.config.delta_t

            trajectory.append(X_BX.detach().cpu().numpy())

        return np.array(trajectory)
    
    def plot_trajectory(self, trajectory):
        """Plot the trajectory of the model"""
        # Use trajectory.shape[0] to ensure t has the right number of points
        t = np.linspace(0, self.config.terminal_time, trajectory.shape[0])
        
        for i in range(self.config.dim_X):
            # Plot each sample separately
            for sample_idx in range(trajectory.shape[1]):
                plt.plot(t, trajectory[:, sample_idx, i])
        
        plt.title('Trajectory of the model')
        plt.xlabel('t')
        plt.ylabel('X(t)')
        plt.grid()
        plt.show()
        
class WholeNet(torch.nn.Module):
    """Building the neural network architecture"""
    def __init__(self):
        super(WholeNet, self).__init__()
        self.config = Config()
        # Initialize p_init as a parameter
        self.p_init = torch.nn.Parameter(
            torch.randn(1, self.config.dim_X, dtype=torch.float32).to(device),  # Ensure float32 and correct device
            requires_grad=True
        )
        self.q_net = FNNetQ()
        self.u_net = FNNetU()
        self.r_nets = torch.nn.ModuleList()
        for _ in range(self.config.dim_L):
            self.r_nets.append(FNNetR())

    def forward(self, sample, x_0_value=1.0, training=True):
        delta_W_TBW, jump_times_BLC, jump_mask_TBLC, jump_sizes_BLCX = sample
        sample_size = delta_W_TBW.shape[1]
        jump_counts = jump_mask_TBLC.shape[3]
        t = np.arange(0, self.config.time_step_count) * self.config.delta_t
        X_BX = self.config.X_init.repeat(sample_size, 1) * x_0_value  # Shape: (sample_size, dim_X)
        p_BX = self.p_init.repeat(sample_size, 1)  # Shape: (sample_size, dim_X)
        r_jump_BLCX = torch.zeros(sample_size, self.config.dim_L, jump_counts, self.config.dim_X, dtype=torch.float32).to(device)  # Shape: (sample_size, dim_L, jump_counts, dim_X)
        r_monte_BLMX = torch.zeros(sample_size, self.config.dim_L, self.config.MC_sample_size, self.config.dim_X, dtype=torch.float32).to(device)  # Shape: (sample_size, dim_L, MC_sample_size, dim_X)
        int_Hu = 0.0  # The constraint term
        MC_sample_points_BLMX = self.config.MC_sample_points_LMX.repeat(sample_size, 1, 1, 1)  # Shape: (sample_size, dim_L, MC_sample_size, dim_X)
        
        for i in range(0, self.config.time_step_count):
            u_BU = self.u_net((t[i], X_BX))   # Shape: (sample_size, dim_u)
            q_BXW = self.q_net((t[i], X_BX))   # Shape: (sample_size, dim_X, dim_W)
            for l in range(self.config.dim_L):
                # Calculate the r_nets for each jump
                r_jump_BLCX[:, l, :, :] = self.r_nets[l]((t[i], X_BX, jump_sizes_BLCX[:, l, :, :]))
                r_monte_BLMX[:, l, :, :] = self.r_nets[l]((t[i], X_BX, MC_sample_points_BLMX[:, l, :, :]))
            
            
            Hu = torch.einsum('bjn,bj->bn', self.config.drift_u(t[i], X_BX, u_BU), p_BX) + \
                torch.einsum('bjkn,bjk->bn', self.config.diffusion_u(t[i], X_BX, u_BU), q_BXW)
            
            for l in range(self.config.dim_L):
                Hu = Hu + self.config.jump_intensity[l] * torch.einsum('bc,bcx->bx', jump_mask_TBLC[i, :, l, :], jump_sizes_BLCX[:, l, :, :] * r_jump_BLCX[:, l, :, :])

            int_Hu = int_Hu + Hu**2 # NOTE: this is where the square was missing! 

            # Update p
            Hx = torch.einsum('bjn,bj->bn', self.config.drift_x(t[i], X_BX, u_BU), p_BX)
            p_BX = p_BX - Hx + torch.einsum('bxw,bw->bx', q_BXW, delta_W_TBW[i, :, :])
            for l in range(self.config.dim_L):
                p_BX = p_BX + torch.einsum('bc,bcx->bx', jump_mask_TBLC[i, :, l, :], r_jump_BLCX[:, l, :, :])
                p_BX = p_BX - self.config.jump_intensity[l] * torch.mean(r_monte_BLMX[:, l, :, :], dim=1) * self.config.delta_t

            # Update X
            X_BX = X_BX + self.config.drift(t[i], X_BX, u_BU) * self.config.delta_t + \
                X_BX * torch.einsum('bxw,bw->bx', self.config.diffusion(t[i], X_BX, u_BU), delta_W_TBW[i, :, :])
            for l in range(self.config.dim_L):
                X_BX = X_BX + u_BU * torch.einsum('bc,bcx->bx', jump_mask_TBLC[i, :, l, :], jump_sizes_BLCX[:, l, :, :])
                X_BX = X_BX - self.config.jump_intensity[l] * (np.exp(self.config.log_normal_mu[l] + 0.5 * self.config.log_normal_sigma[l] ** 2) - 1)* u_BU * self.config.delta_t

        terminal_value_loss = p_BX - self.config.g_x(X_BX)
        loss = torch.mean(torch.sum(terminal_value_loss**2, 1, keepdim=True) + LAMBDA * int_Hu)
        ratio = torch.mean(torch.sum(terminal_value_loss**2, 1, keepdim=True)) / torch.mean(int_Hu)

        performance = torch.mean(self.config.g(X_BX))

        return loss, performance, ratio

class FNNetQ(torch.nn.Module):
    """ Define the feedforward neural network """
    def __init__(self):
        super(FNNetQ, self).__init__()
        self.config = Config()
        num_hiddens = [self.config.dim_X + 10, 
                       self.config.dim_X * 2 + 10, 
                       self.config.dim_X * self.config.dim_W + 10]
        
        # Create layer lists 
        self.bn_layers = torch.nn.ModuleList([
            torch.nn.BatchNorm1d(
                1 + self.config.dim_X,  # Shape of input (t, X)
                momentum=0.99,
                eps=1e-6
            )
        ])
        
        self.dense_layers = torch.nn.ModuleList()
        
        # Hidden layers
        for i in range(len(num_hiddens)):
            input_size = 1 + self.config.dim_X if i == 0 else num_hiddens[i-1]
            self.dense_layers.append(
                torch.nn.Linear(input_size, num_hiddens[i], bias=False)
            )
            self.bn_layers.append(
                torch.nn.BatchNorm1d(
                    num_hiddens[i],
                    momentum=0.99,
                    eps=1e-6
                )
            )
        
        # Output layer: output dim_X * dim_W values per sample
        self.dense_layers.append(
            torch.nn.Linear(num_hiddens[-1], self.config.dim_X * self.config.dim_W)
        )
        self.bn_layers.append(
            torch.nn.BatchNorm1d(
                self.config.dim_X * self.config.dim_W,
                momentum=0.99,
                eps=1e-6
            )
        )
        
        # Initialize weights similar to TF
        for i, module in enumerate(self.modules()):
            if isinstance(module, torch.nn.BatchNorm1d):
                torch.nn.init.normal_(module.weight, 0.3, 0.2)
                torch.nn.init.normal_(module.bias, 0.0, 0.1)

    def forward(self, inputs):
        """structure: bn -> (dense -> bn -> relu) * len(num_hiddens) -> dense -> bn"""
        t, x = inputs
        ts = torch.ones(x.shape[0], 1, dtype=torch.float32).to(x.device) * t  # Ensure correct shape and device
        x = torch.cat([ts, x], dim=1)  # Concatenate along the feature dimension
        x = self.bn_layers[0](x)
        
        for i in range(len(self.dense_layers) - 1):
            x = self.dense_layers[i](x)
            x = self.bn_layers[i+1](x)
            x = torch.nn.functional.relu(x)
        
        x = self.dense_layers[-1](x)
        x = self.bn_layers[-1](x)
        # Reshape to (batch_size, dim_X, dim_W)
        x = x.view(x.shape[0], self.config.dim_X, self.config.dim_W)
        return x

class FNNetU(torch.nn.Module):
    """ Define the feedforward neural network """
    def __init__(self):
        super(FNNetU, self).__init__()
        self.config = Config()
        num_hiddens = [self.config.dim_X+10, self.config.dim_X+10, self.config.dim_X+10]
        
        # Create layer lists 
        self.bn_layers = torch.nn.ModuleList([
            torch.nn.BatchNorm1d(
                1 + self.config.dim_X,  # Shape of input (t, X)
                momentum=0.99,
                eps=1e-6
            )
        ])
        
        self.dense_layers = torch.nn.ModuleList()
        
        # Hidden layers
        for i in range(len(num_hiddens)):
            # Input size for the first layer
            input_size = 1 + self.config.dim_X if i == 0 else num_hiddens[i-1]
            
            self.dense_layers.append(
                torch.nn.Linear(input_size, num_hiddens[i], bias=False)
            )
            
            self.bn_layers.append(
                torch.nn.BatchNorm1d(
                    num_hiddens[i],
                    momentum=0.99,
                    eps=1e-6
                )
            )
        
        # Output layer
        self.dense_layers.append(
            torch.nn.Linear(num_hiddens[-1], self.config.dim_u)
        )
        
        self.bn_layers.append(
            torch.nn.BatchNorm1d(
                self.config.dim_u,
                momentum=0.99,
                eps=1e-6
            )
        )
        
        # Initialize weights similar to TF
        for i, module in enumerate(self.modules()):
            if isinstance(module, torch.nn.BatchNorm1d):
                torch.nn.init.normal_(module.weight, 0.3, 0.2)
                torch.nn.init.normal_(module.bias, 0.0, 0.1)

    def forward(self, inputs):
        """structure: bn -> (dense -> bn -> relu) * len(num_hiddens) -> dense -> bn"""
        t, x = inputs
        ts = torch.ones(x.shape[0], 1, dtype=torch.float32).to(x.device) * t  # Ensure correct shape and device
        x = torch.cat([ts, x], dim=1)  # Concatenate along the feature dimension
        x = self.bn_layers[0](x)
        
        for i in range(len(self.dense_layers) - 1):
            x = self.dense_layers[i](x)
            x = self.bn_layers[i+1](x)
            x = torch.nn.functional.relu(x)
        
        x = self.dense_layers[-1](x)
        x = self.bn_layers[-1](x)
        return x
    
class FNNetR(torch.nn.Module):
    """ Define the feedforward neural network """
    def __init__(self):
        super(FNNetR, self).__init__()
        self.config = Config()
        num_hiddens = [self.config.dim_X * 2 + 10, 
                       self.config.dim_X * 4 + 10, 
                       self.config.dim_X * 2 + 10]
        
        # Create layer lists 
        self.bn_layers = torch.nn.ModuleList([
            torch.nn.BatchNorm1d(
                1 + self.config.dim_X * 2,  # Shape of input (t, X, z)
                momentum=0.99,
                eps=1e-6
            )
        ])
        
        self.dense_layers = torch.nn.ModuleList()
        
        # Hidden layers
        for i in range(len(num_hiddens)):
            input_size = 1 + self.config.dim_X * 2 if i == 0 else num_hiddens[i-1]
            self.dense_layers.append(
                torch.nn.Linear(input_size, num_hiddens[i], bias=False)
            )
            self.bn_layers.append(
                torch.nn.BatchNorm1d(
                    num_hiddens[i],
                    momentum=0.99,
                    eps=1e-6
                )
            )
        
        # Output layer: output dim_X values per sample
        self.dense_layers.append(
            torch.nn.Linear(num_hiddens[-1], self.config.dim_X)
        )
        self.bn_layers.append(
            torch.nn.BatchNorm1d(
                self.config.dim_X,
                momentum=0.99,
                eps=1e-6
            )
        )
        
        # Initialize weights similar to TF
        for i, module in enumerate(self.modules()):
            if isinstance(module, torch.nn.BatchNorm1d):
                torch.nn.init.normal_(module.weight, 0.3, 0.2)
                torch.nn.init.normal_(module.bias, 0.0, 0.1)

    def forward(self, inputs):
        """structure: bn -> (dense -> bn -> relu) * len(num_hiddens) -> dense -> bn"""
        t, x, z = inputs
        # Shape of z: (batch_size, jump_counts, dim_X)

        # Reshape t: (1) -> (batch_size, jump_counts, 1)
        t_ = torch.full((z.shape[0], z.shape[1], 1), t, dtype=torch.float32, device=device)  

        # Reshape x: (batch_size, dim_X) -> (batch_size, jump_counts, dim_X)
        x_ = x.unsqueeze(1).expand(-1, z.shape[1], -1)  

        # Shape of inputs: (batch_size, jump_counts, dim_X * 2 + 1)
        inputs = torch.cat([t_, x_, z], dim=2)

        # Reshape to (batch_size * np.max(jump_counts), dim_X * 2 + 1)
        inputs = inputs.view(-1, inputs.shape[2])

        inputs = self.bn_layers[0](inputs)
        
        for i in range(len(self.dense_layers) - 1):
            inputs = self.dense_layers[i](inputs)
            inputs = self.bn_layers[i+1](inputs)
            inputs = torch.nn.functional.relu(inputs)
        
        inputs = self.dense_layers[-1](inputs)
        inputs = self.bn_layers[-1](inputs)
        # Reshape to (batch_size, jump_counts, dim_X)
        return inputs.view(z.shape[0], z.shape[1], x.shape[1])




def main():
    # Set the random seed for reproducibility   
    torch.manual_seed(42)
    config = Config()
    print("starting the code")
    print("target mean A: ", config.TARGET_MEAN_A)
    
    # config.sample_stock_price(sample_size=10)
    x_0_values = np.array([70, 80, 90, 100, 110, 120])
    V_for_different_x0 = []
    std_for_different_x0 = []
    for x_0 in x_0_values:
        print('\n\n\n x_0: ', x_0)
        # Initialize the solver with the initial value of X0
        solver = Solver(x_0_value=x_0)
        solver.train()
        # generate the trajectory
        trajectory = solver.generate_trajectoy(256, x_0_value=x_0)
        # plot the trajectory
        
        solver.plot_trajectory(trajectory)
        
        # print('Trajectory shape after the plot: ', trajectory.shape)
        # compute the cost functional for each of the trajectories. get the mean and std
        cost_functional = np.mean(np.sum((trajectory - TARGET_MEAN_A) ** 2, axis=2), axis=0) 
        print("cost finctional shape: ", cost_functional.shape)
        print('Cost functional: ', cost_functional)
        print('Mean: ', np.mean(cost_functional))
        print('Std: ', np.std(cost_functional))
        V_for_different_x0.append(np.mean(cost_functional))
        std_for_different_x0.append(np.std(cost_functional))
    
    ### plot the cost functional for different x_0 values. add a transparent area for +-1 std
    plt.figure()
    plt.plot(x_0_values, V_for_different_x0, label='Mean Cost Functional')
    plt.fill_between(x_0_values, 
                     np.array(V_for_different_x0) - np.array(std_for_different_x0), 
                     np.array(V_for_different_x0) + np.array(std_for_different_x0), 
                     alpha=0.2, label='1 Std Dev')
    plt.title('Cost Functional for Different Initial Values of X0')
    plt.xlabel('Initial Value of X0')
    plt.ylabel('Cost Functional')
    plt.legend()
    plt.grid()
    plt.show()
    print(" trying the better time scaling")
    print(f" num_iterations = 200 ,  LAMBDA =  {LAMBDA}, B = 512, A = {TARGET_MEAN_A}")
    print("Cost Functional for different x_0 values:")
    print("Mean: \n", V_for_different_x0)
    print("Std: \n", std_for_different_x0)
    # save the plot
    plt.savefig('cost_functional_2.png')
    plt.close()
    

if __name__ == '__main__':
    main()